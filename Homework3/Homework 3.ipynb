{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nicholas Colonna\n",
    "## Homework 3\n",
    "### \"I pledge my honor that I have abided by the Stevens Honor System.\" -Nicholas Colonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>0.159837</td>\n",
       "      <td>3.514</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.712</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.712</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.153728</td>\n",
       "      <td>1.178</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction\n",
       "0  1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down\n",
       "1  1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down\n",
       "2  1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514        Up\n",
       "3  1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712        Up\n",
       "4  1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178        Up"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "df = pd.read_csv(\"./Weekly.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n",
    "\n",
    "You can observe through some numerical summaries that there are typically more 'Up' weeks than 'Down.' Therefore, you can see that the average weekly return is positive, although small.\n",
    "\n",
    "You can observe from the data that returns generally stay close to 0, but generally fluctuate between 5% and -5%. However, you can see that during years that there are recessions, the weekly returns become much more extreme in both directions, spanning form -20% to 15%.\n",
    "\n",
    "You can also observe that volume has increased significantly since 1990. This can be attributed to the rise of technology and electronic trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Volume: 1.5746176255280073\n",
      "Average Return: 0.1498989898989899\n",
      "\n",
      "Number of Up Weeks: 605 (55.56%)\n",
      "Number of Down Weeks: 484 (44.44%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Average Volume:\", statistics.mean(df['Volume']))\n",
    "print(\"Average Return:\", statistics.mean(df['Today']))\n",
    "print()\n",
    "\n",
    "num_up = 0\n",
    "num_down = 0\n",
    "for direction in df['Direction']:\n",
    "    if direction=='Up':\n",
    "        num_up+=1\n",
    "    else:\n",
    "        num_down+=1\n",
    "print(\"Number of Up Weeks: \",num_up,\" (\",round(100*num_up/(num_up+num_down),2),\"%)\", sep='')\n",
    "print(\"Number of Down Weeks: \",num_down,\" (\",round(100*num_down/(num_up+num_down),2),\"%)\", sep='')\n",
    "print()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df['Year'], df['Today'], 'ro')\n",
    "plt.axis([1988, 2012, -20, 20])\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Return')\n",
    "plt.title('Weekly Returns')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df['Year'], df['Volume'], 'bo')\n",
    "plt.axis([1988, 2012, -1, 10])\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Volume')\n",
    "plt.title('Weekly Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the full data set to perform a logistic regression (baseline algorithm) with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n",
    "\n",
    "First, I created a new column 'Direction_ind' which assigned the value 1 if the variable 'Direction' was equal to \"Up\" and 0 if it was \"Down.\" This will allow us to run our regression.\n",
    "\n",
    "After running the regression, it is apparent that the variables Lag2 and Volume are statistically significant (p-value < 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.686896\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          Direction_ind   No. Observations:                 1089\n",
      "Model:                          Logit   Df Residuals:                     1083\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Tue, 09 Oct 2018   Pseudo R-squ.:               9.505e-05\n",
      "Time:                        15:48:08   Log-Likelihood:                -748.03\n",
      "converged:                       True   LL-Null:                       -748.10\n",
      "                                        LLR p-value:                    0.9996\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Lag1          -0.0327      0.026     -1.250      0.211      -0.084       0.019\n",
      "Lag2           0.0682      0.027      2.556      0.011       0.016       0.120\n",
      "Lag3          -0.0081      0.026     -0.306      0.759      -0.060       0.044\n",
      "Lag4          -0.0194      0.026     -0.740      0.459      -0.071       0.032\n",
      "Lag5          -0.0069      0.026     -0.261      0.794      -0.058       0.045\n",
      "Volume         0.0569      0.027      2.125      0.034       0.004       0.109\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "df['Direction_ind'] = (df['Direction'] == \"Up\").astype(int)\n",
    "\n",
    "y = df['Direction_ind']\n",
    "X = df[['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume']]\n",
    "\n",
    "logistic_reg = sm.Logit(y, X).fit()\n",
    "print(logistic_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n",
    "\n",
    "As you can see from the confusion matrix, there are 172 data points where the model correctly predicted Down. There were also 427 times when the model correctly predicted Up. However, you will notice that there were 312 times when the model falsely identified Up when the true data was Down, and also 178 times when the model falsely identified Down when the true data was Up. With the large amount of misclassification, it leads me to believe that this model does a poor job of differentiating points that are close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted Down  Predicted Up\n",
      "Actual Down             172           312\n",
      "Actual Up               178           427\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.4914    0.3554    0.4125       484\n",
      "         Up     0.5778    0.7058    0.6354       605\n",
      "\n",
      "avg / total     0.5394    0.5500    0.5363      1089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "direct_predict = logistic_reg.predict()\n",
    "predict_classify = [\"Up\" if x>0.5 else \"Down\" for x in direct_predict]\n",
    "conf_all = pd.DataFrame(confusion_matrix(df['Direction'], predict_classify))\n",
    "conf_all = conf_all.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_all)\n",
    "print()\n",
    "print(classification_report(df['Direction'], predict_classify, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n",
    "\n",
    "We observe that the p-value for Lag2 is significant at predicting the Direction of our returns from 1990 to 2008. Once we apply that model to our test data, from 2009 to 2010, we were able to see how well our model is at predicting. The model correctly classified 57 data points and falsely classified 47. Again, this model isn't great at predicting points close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690654\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          Direction_ind   No. Observations:                  985\n",
      "Model:                          Logit   Df Residuals:                      984\n",
      "Method:                           MLE   Df Model:                            0\n",
      "Date:                Tue, 09 Oct 2018   Pseudo R-squ.:               -0.004340\n",
      "Time:                        15:48:12   Log-Likelihood:                -680.29\n",
      "converged:                       True   LL-Null:                       -677.35\n",
      "                                        LLR p-value:                       nan\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Lag2           0.0629      0.029      2.192      0.028       0.007       0.119\n",
      "==============================================================================\n",
      "\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down              20            23\n",
      "Actual Up                24            37\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.4545    0.4651    0.4598        43\n",
      "         Up     0.6167    0.6066    0.6116        61\n",
      "\n",
      "avg / total     0.5496    0.5481    0.5488       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_logit = sm.Logit(df[:-104]['Direction_ind'], df[:-104]['Lag2']).fit()\n",
    "print(trained_logit.summary())\n",
    "print()\n",
    "\n",
    "logit_predict = trained_logit.predict(df[-104:]['Lag2'])\n",
    "logit_predict_classify = [\"Up\" if x>0.5 else \"Down\" for x in logit_predict]\n",
    "conf_logit = pd.DataFrame(confusion_matrix(df[-104:]['Direction'], logit_predict_classify))\n",
    "conf_logit = conf_logit.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_logit)\n",
    "print()\n",
    "print(classification_report(df[-104:]['Direction'], logit_predict_classify, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Repeat (d) using linear discriminant analysis (LDA). This method uses the Bayes classifier with a Gaussian distribution. Even though the discriminant functions are linear functions of the inputs (x), the combination of several classifiers may lead to classification of non-linear problems.\n",
    "\n",
    "After generating a confusion matrix for our LDA, we obtained interesting results. The model correctly classified 9 Down data points and 56 Up points, however, it misclassified 34 Down data points as Up and 5 Up data points as Down. It appears that the LDA model classified a majority of our data as Up, which led to a lot of misclassification and a low recall score for down. However, this model correctly predicted 65 directions in comparison to the logistic model's 57, and had a higher avg/total precision, recall and f1-score, therefore it may be a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted Down  Predicted Up\n",
      "Actual Down               9            34\n",
      "Actual Up                 5            56\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.6429    0.2093    0.3158        43\n",
      "         Up     0.6222    0.9180    0.7417        61\n",
      "\n",
      "avg / total     0.6308    0.6250    0.5656       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_lag2 = np.array(df[:-104]['Lag2'])\n",
    "out_lag2 = np.array(df[-104:]['Lag2'])\n",
    "trained_lda = LinearDiscriminantAnalysis().fit(trained_lag2.reshape(-1,1), df[:-104]['Direction_ind'])\n",
    "\n",
    "lda_predict = trained_lda.predict(out_lag2.reshape(-1,1))\n",
    "lda_predict_classify = [\"Up\" if x>0.5 else \"Down\" for x in lda_predict]\n",
    "conf_lda = pd.DataFrame(confusion_matrix(df[-104:]['Direction'], lda_predict_classify))\n",
    "conf_lda = conf_lda.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_lda)\n",
    "print()\n",
    "\n",
    "print(classification_report(df[-104:]['Direction'], lda_predict_classify, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Repeat (d) using quadratic discriminant analysis (QDA)\n",
    "\n",
    "The QDA model predicted that all of our future data points would be Up, and therefore accurately predicted 61 data points, but misclassified all 43 down points. For these reasons, we see a very low precision score and low f1-score. Although this model accurately predicted 61 points, since it only predicts up as our future values, it will not be a good model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted Down  Predicted Up\n",
      "Actual Down               0            43\n",
      "Actual Up                 0            61\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.0000    0.0000    0.0000        43\n",
      "         Up     0.5865    1.0000    0.7394        61\n",
      "\n",
      "avg / total     0.3440    0.5865    0.4337       104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colon\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "trained_qda = QuadraticDiscriminantAnalysis().fit(trained_lag2.reshape(-1,1), df[:-104]['Direction_ind'])\n",
    "\n",
    "qda_predict = trained_qda.predict(out_lag2.reshape(-1,1))\n",
    "qda_predict_classify = [\"Up\" if x>0.5 else \"Down\" for x in qda_predict]\n",
    "conf_qda = pd.DataFrame(confusion_matrix(df[-104:]['Direction'], qda_predict_classify))\n",
    "conf_qda = conf_qda.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_qda)\n",
    "print()\n",
    "print(classification_report(df[-104:]['Direction'], qda_predict_classify, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Repeat (d) using K-NN with K = 1.\n",
    "\n",
    "Our KNN model accurately predicted 21 Down points and 30 Up points, however, it misclassified 22 Down points as Up and 31 Up points as Down. This means the model was able to accurately predict just below 50% of our data points. This leads to a total precision, recall and f1-score around 0.5. Since these numbers are less than those of the logistic and LDA model, I would not choose the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted Down  Predicted Up\n",
      "Actual Down              21            22\n",
      "Actual Up                31            30\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.4038    0.4884    0.4421        43\n",
      "         Up     0.5769    0.4918    0.5310        61\n",
      "\n",
      "avg / total     0.5054    0.4904    0.4942       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_knn = KNeighborsClassifier(n_neighbors=1).fit(trained_lag2.reshape(-1,1), df[:-104]['Direction_ind'])\n",
    "\n",
    "knn_predict = trained_knn.predict(out_lag2.reshape(-1,1))\n",
    "knn_predict_classify = [\"Up\" if x>0.5 else \"Down\" for x in knn_predict]\n",
    "conf_knn = pd.DataFrame(confusion_matrix(df[-104:]['Direction'], knn_predict_classify))\n",
    "conf_knn = conf_knn.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_knn)\n",
    "print()\n",
    "print(classification_report(df[-104:]['Direction'], knn_predict_classify, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Which of these methods appears to provide the best results on this data?\n",
    "\n",
    "The model with the highest avg / total for f1-score will be the one that produces the best results. After comparing all of the models, it is clear that the LDA was the best, with an f1-score of 0.5656. This means the model does decent with prediction while also having decent recall. The second best model results were from the logistical regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the K-NN classifier.\n",
    "\n",
    "After playing around with the various methods and variables, I arrived at 3 improved models from their original form. First, I was able to improve the logistical model by using the 3 variables Lag1, Lag2, and Lag3. This yielded a better confusion matrix and a total f1-score of 0.5699.\n",
    "\n",
    "I was able to improve the LDA model by adding the variables Lag3 and Volume to the original model of Lag2. With these new variables, the confusion matrix improved and we got a better f1-score of 0.5704. This model is better than the original LDA.\n",
    "\n",
    "Lastly, I was able to improve the KNN model and I got the best results out of all models. I used K=100 and I added the variables Lag1 and Lag3 to the orignal predictors of just Lag2. As a result, there were 63 correctly classified data points and only 41 misclassified. This yielded strong prediction and recall numbers and an improved f1-score of 0.5963."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.689186\n",
      "         Iterations 4\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down              23            20\n",
      "Actual Up                25            36\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.4792    0.5349    0.5055        43\n",
      "         Up     0.6429    0.5902    0.6154        61\n",
      "\n",
      "avg / total     0.5752    0.5673    0.5699       104\n",
      "\n",
      "\n",
      "\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down              24            19\n",
      "Actual Up                26            35\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.4800    0.5581    0.5161        43\n",
      "         Up     0.6481    0.5738    0.6087        61\n",
      "\n",
      "avg / total     0.5786    0.5673    0.5704       104\n",
      "\n",
      "\n",
      "\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down              18            25\n",
      "Actual Up                16            45\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Down     0.5294    0.4186    0.4675        43\n",
      "         Up     0.6429    0.7377    0.6870        61\n",
      "\n",
      "avg / total     0.5960    0.6058    0.5963       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x2 = ['Lag1', 'Lag2', 'Lag3',]\n",
    "trained_logit2 = sm.Logit(df[:-104]['Direction_ind'], df[:-104][x2]).fit()\n",
    "\n",
    "logit_predict2 = trained_logit2.predict(df[-104:][x2])\n",
    "logit_predict2_classify = [\"Up\" if x>0.5 else \"Down\" for x in logit_predict2]\n",
    "conf_logit2 = pd.DataFrame(confusion_matrix(df[-104:]['Direction'], logit_predict2_classify))\n",
    "conf_logit2 = conf_logit2.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_logit2)\n",
    "print()\n",
    "print(classification_report(df[-104:]['Direction'], logit_predict2_classify, digits=4))\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "x3 = ['Lag2', 'Lag3', 'Volume']\n",
    "trained_lda2 = LinearDiscriminantAnalysis().fit(df[:-104][x3], df[:-104]['Direction_ind'])\n",
    "\n",
    "lda_predict2 = trained_lda2.predict(df[-104:][x3])\n",
    "lda_predict2_classify = [\"Up\" if x>0.5 else \"Down\" for x in lda_predict2]\n",
    "conf_lda2 = pd.DataFrame(confusion_matrix(df[-104:]['Direction'], lda_predict2_classify))\n",
    "conf_lda2 = conf_lda2.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_lda2)\n",
    "print()\n",
    "print(classification_report(df[-104:]['Direction'], lda_predict2_classify, digits=4))\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "x4 = ['Lag1','Lag2','Lag3']\n",
    "trained_knn2 = KNeighborsClassifier(n_neighbors=100).fit(df[:-104][x4], df[:-104]['Direction_ind'])\n",
    "\n",
    "knn_predict2 = trained_knn2.predict(df[-104:][x4])\n",
    "knn_predict2_classify = [\"Up\" if x>0.5 else \"Down\" for x in knn_predict2]\n",
    "conf_knn2 = pd.DataFrame(confusion_matrix(df[-104:]['Direction'], knn_predict2_classify))\n",
    "conf_knn2 = conf_knn2.rename(index={0:'Actual Down', 1:'Actual Up'}, columns={0:'Predicted Down', 1:'Predicted Up'})\n",
    "print(conf_knn2)\n",
    "print()\n",
    "print(classification_report(df[-104:]['Direction'], knn_predict2_classify, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
